{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eegIDZuDOkqf"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import math\n",
        "\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KZ0vV_kNi-wH"
      },
      "outputs": [],
      "source": [
        "# COCO class names\n",
        "\n",
        "class_names = [\"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"airplane\" , \"bus\" , \"train\" , \"truck\" , \"boat\" , \"traffic light\" , \"fire hydrant\" , \"street sign\" , \"stop sign\" , \"parking meter\" , \"bench\" , \"bird\" , \"cat\" , \"dog\" , \"horse\" , \"sheep\" , \"cow\" , \"elephant\" , \"bear\" , \"zebra\" , \"giraffe\" , \"hat\" , \"backpack\" , \"umbrella\" , \"shoe\" , \"eye glasses\" , \"handbag\" , \"tie\" , \"suitcase\" ,\n",
        "\"frisbee\" , \"skis\" , \"snowboard\" , \"sports ball\" , \"kite\" , \"baseball bat\" ,\n",
        "\"baseball glove\" , \"skateboard\" , \"surfboard\" , \"tennis racket\" , \"bottle\" ,\n",
        "\"plate\" , \"wine glass\" , \"cup\" , \"fork\" , \"knife\" , \"spoon\" , \"bowl\" ,\n",
        "\"banana\" , \"apple\" , \"sandwich\" , \"orange\" , \"broccoli\" , \"carrot\" , \"hot dog\" ,\n",
        "\"pizza\" , \"donut\" , \"cake\" , \"chair\" , \"couch\" , \"potted plant\" , \"bed\" ,\n",
        "\"mirror\" , \"dining table\" , \"window\" , \"desk\" , \"toilet\" , \"door\" , \"tv\" ,\n",
        "\"laptop\" , \"mouse\" , \"remote\" , \"keyboard\" , \"cell phone\" , \"microwave\" ,\n",
        "\"oven\" , \"toaster\" , \"sink\" , \"refrigerator\" , \"blender\" , \"book\" ,\n",
        "\"clock\" , \"vase\" , \"scissors\" , \"teddy bear\" , \"hair drier\" , \"toothbrush\" , \"hair brush\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = T.Compose([T.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29nZdXmoPUWZ",
        "outputId": "89130df8-2ed7-41d7-d4a4-aff7009af039"
      },
      "outputs": [],
      "source": [
        "# Load faster rcnn\n",
        "faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODFcd5QeQGwj",
        "outputId": "fc697c61-a087-4004-de59-c48b0fb47c56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\MSI\\anaconda3\\envs\\imageD\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
          ]
        }
      ],
      "source": [
        "# Load single shot detector\n",
        "ssd = torchvision.models.detection.ssd300_vgg16(pretrained = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSha7mOxQa3U",
        "outputId": "5bbc133c-9780-456b-8dee-cfa419983241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# put to evaluation mode to not track gradients - just for inference\n",
        "faster_rcnn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntJsdAakQda0",
        "outputId": "8b023299-262e-45e9-de3e-520ae4f51c93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SSD(\n",
              "  (backbone): SSDFeatureExtractorVGG(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): ReLU(inplace=True)\n",
              "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (15): ReLU(inplace=True)\n",
              "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (18): ReLU(inplace=True)\n",
              "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (20): ReLU(inplace=True)\n",
              "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (22): ReLU(inplace=True)\n",
              "    )\n",
              "    (extra): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): Sequential(\n",
              "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (4): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05], steps=[8, 16, 32, 64, 100, 300])\n",
              "  (head): SSDHead(\n",
              "    (classification_head): SSDClassificationHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Conv2d(512, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(1024, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(512, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): Conv2d(256, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (5): Conv2d(256, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (regression_head): SSDRegressionHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n",
              "      Resize(min_size=(300,), max_size=300, mode='bilinear')\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# put to evaluation mode to not track gradients - just for inference\n",
        "ssd.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCqRTt8ULrF",
        "outputId": "02a82c62-dcce-4809-f276-8134a71ba71c"
      },
      "outputs": [],
      "source": [
        "# load yolov8 large model\n",
        "yolo = YOLO('yolov8l.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C8bBANPVmSY"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87mJ-_6cVqcC"
      },
      "source": [
        "# **Detection on Video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oYBFAfpoQe8V"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from time import time\n",
        "\n",
        "# Open the video file for processing\n",
        "cap = cv2.VideoCapture(\"data/video.mp4\")\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_fr = {\n",
        "    'inference_time': [],  # To store the time taken for inference\n",
        "    'conf_scores': [],     # To store confidence scores of detections\n",
        "    'class_names': [],     # To store class names for each detection\n",
        "    'bboxes': []           # To store bounding boxes of detections\n",
        "}\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the video\n",
        "    success, img = cap.read()\n",
        "\n",
        "    # Break the loop if no frame is captured (end of video)\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    # Apply transformations to the frame (e.g., resizing, normalization)\n",
        "    transformed_img = transform(img)\n",
        "\n",
        "    # Start timing the inference process\n",
        "    start_time = time()\n",
        "    \n",
        "    # Perform object detection on the transformed image\n",
        "    pred = faster_rcnn([transformed_img])\n",
        "    \n",
        "    # End timing the inference process\n",
        "    end_time = time()\n",
        "\n",
        "    # Extract bounding boxes, labels, and scores from the prediction\n",
        "    bboxes, labels, scores = pred[0]['boxes'], pred[0]['labels'], pred[0]['scores']\n",
        "\n",
        "    # Filter out predictions with scores below the threshold (0.8 in this case)\n",
        "    num = torch.argwhere(scores > 0.8).shape[0]\n",
        "\n",
        "    # Initialize lists to store results for the current frame\n",
        "    classes = []\n",
        "    boxes = []\n",
        "    conf_scores = []\n",
        "\n",
        "    for i in range(num):\n",
        "        # Convert bounding box coordinates to integers\n",
        "        x1, y1, x2, y2 = bboxes[i].detach().numpy().astype('int')\n",
        "        boxes.append([x1, y1, x2, y2])\n",
        "        \n",
        "        # Draw the bounding box on the original image\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
        "        \n",
        "        # Get the class name from the label\n",
        "        class_name = class_names[labels.detach().numpy()[i] - 1]\n",
        "        classes.append(class_name)\n",
        "        \n",
        "        # Calculate and round the confidence score\n",
        "        conf_score = round(scores.detach().numpy()[i] * 100)\n",
        "        conf_scores.append(conf_score)\n",
        "        \n",
        "        # Put text with the class name on the image\n",
        "        cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "    # Append results of the current frame to the results dictionary\n",
        "    results_fr['bboxes'].append(boxes)\n",
        "    results_fr['conf_scores'].append(conf_scores)\n",
        "    results_fr['class_names'].append(classes)\n",
        "    results_fr['inference_time'].append(end_time - start_time)\n",
        "\n",
        "    # Display the frame with bounding boxes and labels\n",
        "    cv2.imshow('Frame', img)\n",
        "\n",
        "    # Break the loop if 'c' or 'C' is pressed\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close all OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4flzeignrChG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\MSI\\anaconda3\\envs\\imageD\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
          ]
        }
      ],
      "source": [
        "cap = cv2.VideoCapture(\"data/video.mp4\")\n",
        "\n",
        "results_s = {'inference_time' : [],\n",
        "           'conf_scores' : [],\n",
        "           'class_names' : [],\n",
        "           'bboxes' : []}\n",
        "\n",
        "i = 0\n",
        "\n",
        "while True:\n",
        "  success, img = cap.read()\n",
        "\n",
        "  if not success:\n",
        "    break\n",
        "\n",
        "  transformed_img = transform(img)\n",
        "\n",
        "  start_time = time()\n",
        "  pred = ssd([transformed_img])\n",
        "  end_time = time()\n",
        "\n",
        "  bboxes, labels, scores = pred[0]['boxes'], pred[0]['labels'], pred[0]['scores']\n",
        "\n",
        "  num = torch.argwhere(scores > 0.8).shape[0]\n",
        "\n",
        "  classes = []\n",
        "  boxes = []\n",
        "  conf_scores = []\n",
        "\n",
        "  for i in range(num):\n",
        "    x1, y1, x2, y2 = bboxes[i].detach().numpy().astype('int')\n",
        "    boxes.append([x1, y1, x2, y2])\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
        "    class_name = class_names[labels.detach().numpy()[i] - 1]\n",
        "    classes.append(class_name)\n",
        "    conf_score = round(scores.detach().numpy()[i] * 100)\n",
        "    conf_scores.append(conf_score)\n",
        "    cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "  results_s['bboxes'].append(boxes)\n",
        "  results_s['conf_scores'].append(conf_scores)\n",
        "  results_s['class_names'].append(classes)\n",
        "  results_s['inference_time'].append(end_time - start_time)\n",
        "\n",
        "  cv2.imshow('Frame', img)\n",
        "\n",
        "  key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "  if key == ord('q'):\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inference_time</th>\n",
              "      <th>conf_scores</th>\n",
              "      <th>class_names</th>\n",
              "      <th>bboxes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [inference_time, conf_scores, class_names, bboxes]\n",
              "Index: []"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_yolo_df = pd.DataFrame(results_y) # dataframe for yolo results\n",
        "results_yolo_df.to_csv('results_yolo.csv', index=False)\n",
        "results_yolo_df.tail() # last 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average inference time YOLO: nan\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average inference time YOLO: {results_yolo_df['inference_time'].mean()}\") # average inference time yolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inference_time</th>\n",
              "      <th>conf_scores</th>\n",
              "      <th>class_names</th>\n",
              "      <th>bboxes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>3.837206</td>\n",
              "      <td>[100, 100, 100, 100, 99, 99, 91, 87]</td>\n",
              "      <td>[person, person, person, person, person, perso...</td>\n",
              "      <td>[[598, 150, 635, 260], [261, 158, 288, 267], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>3.102902</td>\n",
              "      <td>[100, 100, 100, 99, 99, 98, 86]</td>\n",
              "      <td>[person, person, person, person, person, perso...</td>\n",
              "      <td>[[259, 158, 288, 267], [598, 149, 633, 261], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>3.208609</td>\n",
              "      <td>[100, 100, 100, 100, 100, 99, 89]</td>\n",
              "      <td>[person, person, person, person, person, perso...</td>\n",
              "      <td>[[597, 149, 631, 261], [201, 176, 235, 266], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>3.206170</td>\n",
              "      <td>[100, 100, 100, 100, 99, 99]</td>\n",
              "      <td>[person, person, person, person, person, person]</td>\n",
              "      <td>[[595, 149, 627, 260], [200, 175, 233, 267], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>3.615480</td>\n",
              "      <td>[100, 100, 100, 99, 99, 97]</td>\n",
              "      <td>[person, person, person, person, person, person]</td>\n",
              "      <td>[[594, 148, 624, 260], [536, 156, 571, 259], [...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     inference_time                           conf_scores  \\\n",
              "276        3.837206  [100, 100, 100, 100, 99, 99, 91, 87]   \n",
              "277        3.102902       [100, 100, 100, 99, 99, 98, 86]   \n",
              "278        3.208609     [100, 100, 100, 100, 100, 99, 89]   \n",
              "279        3.206170          [100, 100, 100, 100, 99, 99]   \n",
              "280        3.615480           [100, 100, 100, 99, 99, 97]   \n",
              "\n",
              "                                           class_names  \\\n",
              "276  [person, person, person, person, person, perso...   \n",
              "277  [person, person, person, person, person, perso...   \n",
              "278  [person, person, person, person, person, perso...   \n",
              "279   [person, person, person, person, person, person]   \n",
              "280   [person, person, person, person, person, person]   \n",
              "\n",
              "                                                bboxes  \n",
              "276  [[598, 150, 635, 260], [261, 158, 288, 267], [...  \n",
              "277  [[259, 158, 288, 267], [598, 149, 633, 261], [...  \n",
              "278  [[597, 149, 631, 261], [201, 176, 235, 266], [...  \n",
              "279  [[595, 149, 627, 260], [200, 175, 233, 267], [...  \n",
              "280  [[594, 148, 624, 260], [536, 156, 571, 259], [...  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_fr_df = pd.DataFrame(results_fr) # dataframe for faster rcnn\n",
        "results_fr_df.to_csv('results_faster_rcnn.csv', index=False)\n",
        "results_fr_df.tail() # last 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average inference time Faster RCNN: 3.5647847593042776\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average inference time Faster RCNN: {results_fr_df['inference_time'].mean()}\") # average inference time faster rcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inference_time</th>\n",
              "      <th>conf_scores</th>\n",
              "      <th>class_names</th>\n",
              "      <th>bboxes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>0.471755</td>\n",
              "      <td>[99, 89]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[543, 160, 584, 261], [203, 174, 240, 268]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>0.376606</td>\n",
              "      <td>[98, 90]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[540, 163, 581, 261], [202, 173, 239, 268]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>0.497371</td>\n",
              "      <td>[97, 87]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[539, 162, 579, 262], [201, 173, 238, 268]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>0.456156</td>\n",
              "      <td>[95, 90]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[537, 159, 576, 263], [589, 150, 627, 257]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>0.320021</td>\n",
              "      <td>[94, 93, 91]</td>\n",
              "      <td>[person, person, person]</td>\n",
              "      <td>[[587, 150, 624, 258], [536, 158, 572, 262], [...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     inference_time   conf_scores               class_names  \\\n",
              "276        0.471755      [99, 89]          [person, person]   \n",
              "277        0.376606      [98, 90]          [person, person]   \n",
              "278        0.497371      [97, 87]          [person, person]   \n",
              "279        0.456156      [95, 90]          [person, person]   \n",
              "280        0.320021  [94, 93, 91]  [person, person, person]   \n",
              "\n",
              "                                                bboxes  \n",
              "276       [[543, 160, 584, 261], [203, 174, 240, 268]]  \n",
              "277       [[540, 163, 581, 261], [202, 173, 239, 268]]  \n",
              "278       [[539, 162, 579, 262], [201, 173, 238, 268]]  \n",
              "279       [[537, 159, 576, 263], [589, 150, 627, 257]]  \n",
              "280  [[587, 150, 624, 258], [536, 158, 572, 262], [...  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_s_df = pd.DataFrame(results_s) # dataframe for ssd\n",
        "results_s_df.to_csv('results_ssd.csv', index=False)\n",
        "results_s_df.tail() # last 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average inference time SSD: 0.5815045859041588\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average inference time SSD: {results_s_df['inference_time'].mean()}\") # average inference time ssd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SDQfG4jVwk_"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM1DEyVoVx3B"
      },
      "source": [
        "# **Tracking with DeepSORT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CuUBoyXtVxCC"
      },
      "outputs": [],
      "source": [
        "from deep_sort.deep_sort import DeepSort\n",
        "\n",
        "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7' # Path to the DeepSort model\n",
        "tracker = DeepSort(model_path=deep_sort_weights, max_age=70) # Initialize the DeepSort tracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Open the video file for processing\n",
        "cap = cv2.VideoCapture(\"data/video.mp4\")\n",
        "\n",
        "# Retrieve and store the width of the video frames\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "# Retrieve and store the height of the video frames\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Retrieve and store the frames per second (FPS) of the video\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # check if GPU is available\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "frames = []  # Initialize an empty list to store the frames\n",
        "unique_track_ids = set()  # Initialize an empty set to store the unique track IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "output_path = 'output1.mp4'\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l.pt to 'yolov8l.pt'...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "536c402e304f4153b19db28ad4bf8b78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/83.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 3 persons, 87.9ms\n",
            "Speed: 4.1ms preprocess, 87.9ms inference, 34.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 70.8ms\n",
            "Speed: 3.0ms preprocess, 70.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 76.5ms\n",
            "Speed: 5.0ms preprocess, 76.5ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 27.5ms\n",
            "Speed: 0.0ms preprocess, 27.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 26.7ms\n",
            "Speed: 0.0ms preprocess, 26.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 25.5ms\n",
            "Speed: 0.0ms preprocess, 25.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 25.4ms\n",
            "Speed: 1.0ms preprocess, 25.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 24.7ms\n",
            "Speed: 1.0ms preprocess, 24.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 24.3ms\n",
            "Speed: 0.9ms preprocess, 24.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 24.8ms\n",
            "Speed: 1.0ms preprocess, 24.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 49.6ms\n",
            "Speed: 2.0ms preprocess, 49.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 19.0ms\n",
            "Speed: 1.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 20.5ms\n",
            "Speed: 1.0ms preprocess, 20.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 36.0ms\n",
            "Speed: 1.0ms preprocess, 36.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 30.0ms\n",
            "Speed: 1.0ms preprocess, 30.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 27.2ms\n",
            "Speed: 1.0ms preprocess, 27.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 24.8ms\n",
            "Speed: 1.9ms preprocess, 24.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 25.6ms\n",
            "Speed: 1.0ms preprocess, 25.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 45.1ms\n",
            "Speed: 1.0ms preprocess, 45.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 19.3ms\n",
            "Speed: 0.0ms preprocess, 19.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 19.0ms\n",
            "Speed: 0.0ms preprocess, 19.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 37.1ms\n",
            "Speed: 1.0ms preprocess, 37.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 20.2ms\n",
            "Speed: 1.1ms preprocess, 20.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 30.6ms\n",
            "Speed: 4.0ms preprocess, 30.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 49.4ms\n",
            "Speed: 1.0ms preprocess, 49.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 66.3ms\n",
            "Speed: 0.6ms preprocess, 66.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 67.2ms\n",
            "Speed: 6.2ms preprocess, 67.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 88.7ms\n",
            "Speed: 2.9ms preprocess, 88.7ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 59.9ms\n",
            "Speed: 0.0ms preprocess, 59.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 51.9ms\n",
            "Speed: 1.1ms preprocess, 51.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 43.6ms\n",
            "Speed: 3.1ms preprocess, 43.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 45.7ms\n",
            "Speed: 1.0ms preprocess, 45.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 21.7ms\n",
            "Speed: 1.0ms preprocess, 21.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 22.0ms\n",
            "Speed: 2.5ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 46.2ms\n",
            "Speed: 2.0ms preprocess, 46.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 20.9ms\n",
            "Speed: 1.0ms preprocess, 20.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 44.2ms\n",
            "Speed: 1.0ms preprocess, 44.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 33.8ms\n",
            "Speed: 1.0ms preprocess, 33.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 55.0ms\n",
            "Speed: 2.1ms preprocess, 55.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 52.1ms\n",
            "Speed: 2.0ms preprocess, 52.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 24.8ms\n",
            "Speed: 1.5ms preprocess, 24.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 51.3ms\n",
            "Speed: 1.0ms preprocess, 51.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 84.2ms\n",
            "Speed: 3.9ms preprocess, 84.2ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 44.3ms\n",
            "Speed: 3.3ms preprocess, 44.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 26.0ms\n",
            "Speed: 1.0ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 51.4ms\n",
            "Speed: 2.9ms preprocess, 51.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 51.3ms\n",
            "Speed: 1.9ms preprocess, 51.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 36.1ms\n",
            "Speed: 2.0ms preprocess, 36.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 45.8ms\n",
            "Speed: 2.0ms preprocess, 45.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 49.7ms\n",
            "Speed: 2.1ms preprocess, 49.7ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 49.8ms\n",
            "Speed: 1.9ms preprocess, 49.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 22.5ms\n",
            "Speed: 1.1ms preprocess, 22.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 52.7ms\n",
            "Speed: 1.0ms preprocess, 52.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 31.2ms\n",
            "Speed: 1.9ms preprocess, 31.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 48.5ms\n",
            "Speed: 1.7ms preprocess, 48.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 54.3ms\n",
            "Speed: 1.0ms preprocess, 54.3ms inference, 15.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 50.8ms\n",
            "Speed: 0.0ms preprocess, 50.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 27.0ms\n",
            "Speed: 3.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 51.0ms\n",
            "Speed: 4.0ms preprocess, 51.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 27.7ms\n",
            "Speed: 1.0ms preprocess, 27.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 22.7ms\n",
            "Speed: 1.0ms preprocess, 22.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 56.2ms\n",
            "Speed: 1.7ms preprocess, 56.2ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 32.4ms\n",
            "Speed: 2.5ms preprocess, 32.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 56.7ms\n",
            "Speed: 2.0ms preprocess, 56.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 28.4ms\n",
            "Speed: 1.0ms preprocess, 28.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 27.6ms\n",
            "Speed: 1.1ms preprocess, 27.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 51.4ms\n",
            "Speed: 5.8ms preprocess, 51.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 25.2ms\n",
            "Speed: 1.1ms preprocess, 25.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 46.8ms\n",
            "Speed: 1.0ms preprocess, 46.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 29.9ms\n",
            "Speed: 1.0ms preprocess, 29.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 50.9ms\n",
            "Speed: 1.0ms preprocess, 50.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 62.9ms\n",
            "Speed: 2.0ms preprocess, 62.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 46.9ms\n",
            "Speed: 1.1ms preprocess, 46.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 46.2ms\n",
            "Speed: 1.2ms preprocess, 46.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 26.0ms\n",
            "Speed: 1.2ms preprocess, 26.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 45.8ms\n",
            "Speed: 2.1ms preprocess, 45.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 62.4ms\n",
            "Speed: 3.0ms preprocess, 62.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 46.3ms\n",
            "Speed: 1.0ms preprocess, 46.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 26.9ms\n",
            "Speed: 2.0ms preprocess, 26.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 53.5ms\n",
            "Speed: 2.7ms preprocess, 53.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 51.8ms\n",
            "Speed: 3.0ms preprocess, 51.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 19.1ms\n",
            "Speed: 2.0ms preprocess, 19.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 28.8ms\n",
            "Speed: 1.1ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 51.8ms\n",
            "Speed: 2.3ms preprocess, 51.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 63.0ms\n",
            "Speed: 2.7ms preprocess, 63.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 29.1ms\n",
            "Speed: 1.9ms preprocess, 29.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 55.4ms\n",
            "Speed: 2.0ms preprocess, 55.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 45.1ms\n",
            "Speed: 0.9ms preprocess, 45.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 20.1ms\n",
            "Speed: 0.9ms preprocess, 20.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 63.0ms\n",
            "Speed: 3.0ms preprocess, 63.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 37.0ms\n",
            "Speed: 2.0ms preprocess, 37.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 70.1ms\n",
            "Speed: 3.0ms preprocess, 70.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 55.4ms\n",
            "Speed: 2.0ms preprocess, 55.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 48.3ms\n",
            "Speed: 2.1ms preprocess, 48.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 28.1ms\n",
            "Speed: 0.9ms preprocess, 28.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 48.2ms\n",
            "Speed: 1.9ms preprocess, 48.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 45.5ms\n",
            "Speed: 4.0ms preprocess, 45.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 44.7ms\n",
            "Speed: 2.1ms preprocess, 44.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 30.3ms\n",
            "Speed: 2.1ms preprocess, 30.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 48.0ms\n",
            "Speed: 1.0ms preprocess, 48.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 52.4ms\n",
            "Speed: 2.0ms preprocess, 52.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 23.1ms\n",
            "Speed: 1.0ms preprocess, 23.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 50.7ms\n",
            "Speed: 2.7ms preprocess, 50.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 26.2ms\n",
            "Speed: 1.0ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 81.5ms\n",
            "Speed: 3.0ms preprocess, 81.5ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 50.8ms\n",
            "Speed: 4.0ms preprocess, 50.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 57.4ms\n",
            "Speed: 1.7ms preprocess, 57.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 49.0ms\n",
            "Speed: 2.1ms preprocess, 49.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 48.6ms\n",
            "Speed: 1.7ms preprocess, 48.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 39.6ms\n",
            "Speed: 2.0ms preprocess, 39.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 66.6ms\n",
            "Speed: 5.5ms preprocess, 66.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 27.9ms\n",
            "Speed: 2.0ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 56.6ms\n",
            "Speed: 2.0ms preprocess, 56.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 59.1ms\n",
            "Speed: 2.0ms preprocess, 59.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 54.5ms\n",
            "Speed: 2.0ms preprocess, 54.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 22.1ms\n",
            "Speed: 1.0ms preprocess, 22.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 38.6ms\n",
            "Speed: 2.6ms preprocess, 38.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 49.7ms\n",
            "Speed: 1.0ms preprocess, 49.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 55.0ms\n",
            "Speed: 1.6ms preprocess, 55.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 51.5ms\n",
            "Speed: 2.0ms preprocess, 51.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 55.0ms\n",
            "Speed: 3.0ms preprocess, 55.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 57.9ms\n",
            "Speed: 0.9ms preprocess, 57.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 40.1ms\n",
            "Speed: 1.6ms preprocess, 40.1ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 51.5ms\n",
            "Speed: 1.9ms preprocess, 51.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 19.7ms\n",
            "Speed: 2.0ms preprocess, 19.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 45.5ms\n",
            "Speed: 2.1ms preprocess, 45.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 19.0ms\n",
            "Speed: 1.1ms preprocess, 19.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 19.4ms\n",
            "Speed: 1.0ms preprocess, 19.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 45.7ms\n",
            "Speed: 1.0ms preprocess, 45.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 24.6ms\n",
            "Speed: 1.9ms preprocess, 24.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 28.0ms\n",
            "Speed: 2.1ms preprocess, 28.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 28.8ms\n",
            "Speed: 0.7ms preprocess, 28.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 27.0ms\n",
            "Speed: 0.0ms preprocess, 27.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 25.5ms\n",
            "Speed: 1.4ms preprocess, 25.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 31.3ms\n",
            "Speed: 0.5ms preprocess, 31.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 26.6ms\n",
            "Speed: 0.5ms preprocess, 26.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 25.7ms\n",
            "Speed: 1.0ms preprocess, 25.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 45.8ms\n",
            "Speed: 1.1ms preprocess, 45.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 23.7ms\n",
            "Speed: 1.0ms preprocess, 23.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 23.3ms\n",
            "Speed: 2.1ms preprocess, 23.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 43.3ms\n",
            "Speed: 1.1ms preprocess, 43.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 20.9ms\n",
            "Speed: 1.0ms preprocess, 20.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 56.8ms\n",
            "Speed: 2.5ms preprocess, 56.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 27.0ms\n",
            "Speed: 1.6ms preprocess, 27.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 61.9ms\n",
            "Speed: 2.9ms preprocess, 61.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 31.0ms\n",
            "Speed: 2.0ms preprocess, 31.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 52.8ms\n",
            "Speed: 1.0ms preprocess, 52.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 22.7ms\n",
            "Speed: 1.3ms preprocess, 22.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 55.3ms\n",
            "Speed: 3.0ms preprocess, 55.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 36.7ms\n",
            "Speed: 1.0ms preprocess, 36.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 78.7ms\n",
            "Speed: 3.0ms preprocess, 78.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 76.5ms\n",
            "Speed: 12.6ms preprocess, 76.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 31.9ms\n",
            "Speed: 1.0ms preprocess, 31.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 48.6ms\n",
            "Speed: 2.9ms preprocess, 48.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 62.5ms\n",
            "Speed: 3.0ms preprocess, 62.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 55.4ms\n",
            "Speed: 2.4ms preprocess, 55.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 39.3ms\n",
            "Speed: 2.0ms preprocess, 39.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 51.9ms\n",
            "Speed: 2.1ms preprocess, 51.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 36.0ms\n",
            "Speed: 2.1ms preprocess, 36.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 31.8ms\n",
            "Speed: 1.0ms preprocess, 31.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 76.3ms\n",
            "Speed: 3.7ms preprocess, 76.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 30.6ms\n",
            "Speed: 1.0ms preprocess, 30.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 28.8ms\n",
            "Speed: 1.9ms preprocess, 28.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 45.2ms\n",
            "Speed: 2.0ms preprocess, 45.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 52.6ms\n",
            "Speed: 1.0ms preprocess, 52.6ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 52.8ms\n",
            "Speed: 2.9ms preprocess, 52.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 31.2ms\n",
            "Speed: 1.0ms preprocess, 31.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 28.0ms\n",
            "Speed: 1.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 50.0ms\n",
            "Speed: 1.0ms preprocess, 50.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 32.0ms\n",
            "Speed: 2.0ms preprocess, 32.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 56.8ms\n",
            "Speed: 1.0ms preprocess, 56.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 24.7ms\n",
            "Speed: 1.9ms preprocess, 24.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 35.3ms\n",
            "Speed: 2.0ms preprocess, 35.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 53.7ms\n",
            "Speed: 1.9ms preprocess, 53.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 26.0ms\n",
            "Speed: 1.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 25.4ms\n",
            "Speed: 4.0ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 47.9ms\n",
            "Speed: 1.6ms preprocess, 47.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 57.7ms\n",
            "Speed: 2.0ms preprocess, 57.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 50.0ms\n",
            "Speed: 2.0ms preprocess, 50.0ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 47.1ms\n",
            "Speed: 1.0ms preprocess, 47.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 21.8ms\n",
            "Speed: 4.2ms preprocess, 21.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 33.2ms\n",
            "Speed: 1.1ms preprocess, 33.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 54.6ms\n",
            "Speed: 1.7ms preprocess, 54.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 27.2ms\n",
            "Speed: 0.8ms preprocess, 27.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 59.4ms\n",
            "Speed: 1.9ms preprocess, 59.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 77.2ms\n",
            "Speed: 3.3ms preprocess, 77.2ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 29.9ms\n",
            "Speed: 1.0ms preprocess, 29.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 64.1ms\n",
            "Speed: 2.0ms preprocess, 64.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 39.3ms\n",
            "Speed: 2.2ms preprocess, 39.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 44.4ms\n",
            "Speed: 2.0ms preprocess, 44.4ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 98.7ms\n",
            "Speed: 1.0ms preprocess, 98.7ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 48.7ms\n",
            "Speed: 2.3ms preprocess, 48.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 28.2ms\n",
            "Speed: 1.9ms preprocess, 28.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 35.0ms\n",
            "Speed: 0.9ms preprocess, 35.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 84.5ms\n",
            "Speed: 4.3ms preprocess, 84.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 51.9ms\n",
            "Speed: 1.0ms preprocess, 51.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 49.6ms\n",
            "Speed: 1.9ms preprocess, 49.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 26.5ms\n",
            "Speed: 1.9ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 53.8ms\n",
            "Speed: 2.0ms preprocess, 53.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 77.8ms\n",
            "Speed: 5.0ms preprocess, 77.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 52.4ms\n",
            "Speed: 2.0ms preprocess, 52.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 26.0ms\n",
            "Speed: 1.0ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 48.6ms\n",
            "Speed: 2.0ms preprocess, 48.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 24.3ms\n",
            "Speed: 1.0ms preprocess, 24.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 54.2ms\n",
            "Speed: 3.0ms preprocess, 54.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 persons, 28.1ms\n",
            "Speed: 1.0ms preprocess, 28.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 66.9ms\n",
            "Speed: 4.1ms preprocess, 66.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 31.3ms\n",
            "Speed: 2.0ms preprocess, 31.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 49.7ms\n",
            "Speed: 2.0ms preprocess, 49.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 33.0ms\n",
            "Speed: 2.0ms preprocess, 33.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 55.4ms\n",
            "Speed: 3.2ms preprocess, 55.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 44.2ms\n",
            "Speed: 2.0ms preprocess, 44.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 61.1ms\n",
            "Speed: 2.0ms preprocess, 61.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 35.3ms\n",
            "Speed: 2.0ms preprocess, 35.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 33.3ms\n",
            "Speed: 2.6ms preprocess, 33.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 56.3ms\n",
            "Speed: 2.1ms preprocess, 56.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 39.5ms\n",
            "Speed: 1.1ms preprocess, 39.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 59.0ms\n",
            "Speed: 3.0ms preprocess, 59.0ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 50.8ms\n",
            "Speed: 2.0ms preprocess, 50.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 33.4ms\n",
            "Speed: 2.1ms preprocess, 33.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 64.5ms\n",
            "Speed: 3.0ms preprocess, 64.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 58.2ms\n",
            "Speed: 3.0ms preprocess, 58.2ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 50.3ms\n",
            "Speed: 2.0ms preprocess, 50.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 45.9ms\n",
            "Speed: 3.0ms preprocess, 45.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 76.7ms\n",
            "Speed: 4.0ms preprocess, 76.7ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 39.3ms\n",
            "Speed: 2.0ms preprocess, 39.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 107.8ms\n",
            "Speed: 4.0ms preprocess, 107.8ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 45.0ms\n",
            "Speed: 1.1ms preprocess, 45.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 53.2ms\n",
            "Speed: 2.6ms preprocess, 53.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 30.6ms\n",
            "Speed: 1.7ms preprocess, 30.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 43.4ms\n",
            "Speed: 2.0ms preprocess, 43.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 37.2ms\n",
            "Speed: 1.0ms preprocess, 37.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 45.1ms\n",
            "Speed: 1.0ms preprocess, 45.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 60.9ms\n",
            "Speed: 2.1ms preprocess, 60.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 58.2ms\n",
            "Speed: 2.0ms preprocess, 58.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 55.0ms\n",
            "Speed: 3.0ms preprocess, 55.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 31.3ms\n",
            "Speed: 2.1ms preprocess, 31.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 43.0ms\n",
            "Speed: 3.0ms preprocess, 43.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 26.9ms\n",
            "Speed: 1.0ms preprocess, 26.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 53.3ms\n",
            "Speed: 2.9ms preprocess, 53.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 76.4ms\n",
            "Speed: 2.0ms preprocess, 76.4ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 121.8ms\n",
            "Speed: 3.1ms preprocess, 121.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 20.2ms\n",
            "Speed: 1.0ms preprocess, 20.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 53.8ms\n",
            "Speed: 2.1ms preprocess, 53.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 22.9ms\n",
            "Speed: 1.0ms preprocess, 22.9ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 54.6ms\n",
            "Speed: 5.9ms preprocess, 54.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 46.4ms\n",
            "Speed: 1.0ms preprocess, 46.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 24.9ms\n",
            "Speed: 2.1ms preprocess, 24.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 71.6ms\n",
            "Speed: 2.0ms preprocess, 71.6ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 23.5ms\n",
            "Speed: 2.0ms preprocess, 23.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 43.2ms\n",
            "Speed: 2.0ms preprocess, 43.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 43.6ms\n",
            "Speed: 2.1ms preprocess, 43.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 30.0ms\n",
            "Speed: 1.0ms preprocess, 30.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 61.4ms\n",
            "Speed: 2.0ms preprocess, 61.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 52.1ms\n",
            "Speed: 2.1ms preprocess, 52.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 68.1ms\n",
            "Speed: 4.0ms preprocess, 68.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 90.5ms\n",
            "Speed: 2.0ms preprocess, 90.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 99.3ms\n",
            "Speed: 4.0ms preprocess, 99.3ms inference, 11.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 31.1ms\n",
            "Speed: 1.0ms preprocess, 31.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 26.1ms\n",
            "Speed: 1.0ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 50.1ms\n",
            "Speed: 5.0ms preprocess, 50.1ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 29.6ms\n",
            "Speed: 2.0ms preprocess, 29.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 89.4ms\n",
            "Speed: 4.1ms preprocess, 89.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 26.1ms\n",
            "Speed: 1.0ms preprocess, 26.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 38.4ms\n",
            "Speed: 2.0ms preprocess, 38.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 58.1ms\n",
            "Speed: 2.0ms preprocess, 58.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 23.8ms\n",
            "Speed: 1.0ms preprocess, 23.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 50.7ms\n",
            "Speed: 2.0ms preprocess, 50.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 34.2ms\n",
            "Speed: 2.0ms preprocess, 34.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 98.4ms\n",
            "Speed: 2.0ms preprocess, 98.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 33.5ms\n",
            "Speed: 2.0ms preprocess, 33.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 65.8ms\n",
            "Speed: 4.1ms preprocess, 65.8ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 51.6ms\n",
            "Speed: 2.1ms preprocess, 51.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 21.7ms\n",
            "Speed: 1.1ms preprocess, 21.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 38.0ms\n",
            "Speed: 2.1ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 48.8ms\n",
            "Speed: 2.7ms preprocess, 48.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 66.4ms\n",
            "Speed: 2.0ms preprocess, 66.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 23.9ms\n",
            "Speed: 0.6ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 27.0ms\n",
            "Speed: 2.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 48.2ms\n",
            "Speed: 2.0ms preprocess, 48.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m results_track \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf_scores\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_names\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_id\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Start reading the video frames\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misOpened\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     16\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# Read a frame from the video\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# Convert the frame from BGR to RGB format (YOLO typically expects RGB)\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Initialize variables for tracking FPS and elapsed time\n",
        "i = 0\n",
        "counter, fps, elapsed = 0, 0, 0\n",
        "start_time = time()  # Record the start time for FPS calculation\n",
        "\n",
        "# Dictionary to store tracking results, including confidence scores, class names, bounding boxes, and track IDs\n",
        "results_track = {\n",
        "    'conf_scores': [],\n",
        "    'class_names': [],\n",
        "    'bboxes': [],\n",
        "    'track_id': []\n",
        "}\n",
        "\n",
        "# Start reading the video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()  # Read a frame from the video\n",
        "\n",
        "    if ret:\n",
        "        # Convert the frame from BGR to RGB format (YOLO typically expects RGB)\n",
        "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = og_frame.copy()\n",
        "\n",
        "        # Load the YOLOv8 model\n",
        "        model = YOLO(\"yolov8l.pt\")\n",
        "\n",
        "        # Run the model on the current frame with specified settings (device, classes, confidence threshold)\n",
        "        results = model(frame, device=0, classes=0, conf=0.8)\n",
        "\n",
        "        # Process the results from YOLO for each detection\n",
        "        for result in results:\n",
        "            boxes = result.boxes  # Extract bounding boxes\n",
        "            cls = boxes.cls.tolist()  # Extract class indices\n",
        "            xyxy = boxes.xyxy  # Extract bounding box coordinates (x1, y1, x2, y2)\n",
        "            conf = boxes.conf  # Extract confidence scores\n",
        "            xywh = boxes.xywh  # Extract bounding box coordinates (x, y, width, height)\n",
        "            \n",
        "            # Map class indices to class names\n",
        "            for class_index in cls:\n",
        "                class_name = class_names[int(class_index)]\n",
        "\n",
        "        # Convert predictions to numpy arrays and detach from computation graph\n",
        "        pred_cls = np.array(cls)\n",
        "        conf = conf.detach().cpu().numpy()\n",
        "        xyxy = xyxy.detach().cpu().numpy()\n",
        "        bboxes_xywh = xywh.cpu().numpy()\n",
        "\n",
        "        # Store the tracking results in the results_track dictionary\n",
        "        results_track['conf_scores'].extend(conf.tolist())\n",
        "        results_track['class_names'].extend([class_names[int(index)] for index in cls])\n",
        "        results_track['bboxes'].extend(xyxy.tolist())\n",
        "        \n",
        "        # Update the tracker with new bounding boxes and confidence scores\n",
        "        tracks = tracker.update(bboxes_xywh, conf, og_frame)\n",
        "\n",
        "        # Iterate over the tracks to draw bounding boxes and track IDs on the frame\n",
        "        for track in tracker.tracker.tracks:\n",
        "            track_id = track.track_id  # Retrieve the track ID\n",
        "            hits = track.hits  # Number of hits for the track (how often it was detected)\n",
        "            x1, y1, x2, y2 = track.to_tlbr()  # Convert bounding box format to (top-left, bottom-right)\n",
        "            w = x2 - x1  # Calculate the width of the bounding box\n",
        "            h = y2 - y1  # Calculate the height of the bounding box\n",
        "\n",
        "            # Define colors for bounding boxes based on track ID\n",
        "            red_color = (0, 0, 255)  # Red color\n",
        "            blue_color = (255, 0, 0)  # Blue color\n",
        "            green_color = (0, 255, 0)  # Green color\n",
        "\n",
        "            # Cycle through colors based on track ID\n",
        "            color_id = track_id % 3\n",
        "            if color_id == 0:\n",
        "                color = red_color\n",
        "            elif color_id == 1:\n",
        "                color = blue_color\n",
        "            else:\n",
        "                color = green_color\n",
        "\n",
        "            # Draw the bounding box with the appropriate color\n",
        "            cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
        "\n",
        "            # Draw the class name and track ID above the bounding box\n",
        "            text_color = (0, 0, 0)  # Black color for text\n",
        "            cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)\n",
        "            \n",
        "            # Add the track ID to the set of unique track IDs\n",
        "            unique_track_ids.add(track_id)\n",
        "\n",
        "        # Count the number of unique persons being tracked\n",
        "        person_count = len(unique_track_ids)\n",
        "\n",
        "        # Update the FPS calculation based on elapsed time\n",
        "        current_time = time()\n",
        "        elapsed = (current_time - start_time)\n",
        "        counter += 1\n",
        "        if elapsed > 1:\n",
        "            fps = counter / elapsed\n",
        "            counter = 0\n",
        "            start_time = current_time\n",
        "\n",
        "        # Display the person count on the frame\n",
        "        cv2.putText(og_frame, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        # Store the processed frame for output\n",
        "        frames.append(og_frame)\n",
        "\n",
        "        # Write the frame to the output video file\n",
        "        out.write(cv2.cvtColor(og_frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Display the frame with bounding boxes and tracking info\n",
        "        cv2.imshow(\"Video\", og_frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit the loop\n",
        "            break\n",
        "\n",
        "# Release resources after processing is done\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame(results_track).to_csv('results_track.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
